---
title: "Text classification and annotation with large language models"
subtitle: Research Methods in Human Sciences
format:
  clean-revealjs:
    self-contained: true
    template-partials:
      - title-slide.html
author:
  - name: Aleksi Knuutila
    orcid: 0000-0002-9874-0079
    email: aleksi.knuutila@helsinki.fi
    affiliations: JYU, HY
date: 2023-09-05
slide_link: http://knuutila.net/llm_methods
---

## A little bit about me 

- University researcher in Helsinki (Sociology) and Jyväskylä (Culture studies)
- Working on information environment in Ukraine, hate speech in Finland
- Experimenting with "distant reading" of Ukraine news with LLMs
- Interested in critical and creative use of machine learning in social sciences

::: {.notes}
Speaker notes go here.

- I've worked on hate speech for a few years now, collaborating with many people in this panel also

- Research approach, based on content analysis, categorisation of large datasets, often from social media
:::

---

## Plan for 2 sessions

- 05.09.2023 12.00 - 14.00: Introduction into LLM analysis and notebook environment for practical exercise

- 07.09.2023 12.00 - 14.00: Practical exercises with text data and group feedback

::: {.notes}
Speaker notes go here.

- I've worked on hate speech for a few years now, collaborating with many people in this panel also

- Research approach, based on content analysis, categorisation of large datasets, often from social media
:::

---

## Structure of presentation in first session

1. What are LLMs
2. Different workflows with LLMs: Supervised categorisation and zero-shot annotation
3. Two examples of LLMs in research
4. Introducing the CSC notebook environment

---

## Learning goals for sessions

- Understand what LLMs are 
- Be able to evaluate whether they will be useful for your research
- Some basic techniques and experiences for text classification and text annotation with LLMs

# What are large language models (LLMs) {background-color="#40666e"}

## Examples of large language models

![](Untitled design.png)

- Google Bert, 2017
- ChatGPT, 2022
- Llama-2, 2023

::: {.notes}
- ChatGPT first language model that became consumer technology
- Open source ones
:::

## Some analogies of LLMs

- Emily Bender et al.: Stochastic parrots
- Ted Chiang: "ChatGPT is a Blurry JPEG of the Web"
- Simon Willison: "Calculators for words"

::: {style="font-size: 50%;"}
Sources: Bender et al. "On the Dangers of Stochastic Parrots: Can Language Models Be Too Big", Ted Chiang in [New Yorker](https://www.newyorker.com/tech/annals-of-technology/chatgpt-is-a-blurry-jpeg-of-the-web), [Simon Willison](https://simonwillison.net/2023/Apr/2/calculator-for-words/)
:::

---

## A technical definition for LLMs

![](Pasted image 20230825094408.png)

- LLMs model text and can be used to generate it
- LLMs receive large-scale pretraining
- LLMs make inferences based on transfer learning

::: {style="font-size: 50%;"}
Source: Luccioni, Alexandra Sasha, and Anna Rogers. “Mind Your Language (Model): Fact-Checking LLMs and Their Role in NLP Research and Practice.”
:::

::: {.notes}
- Pre-training: Large organisations
- They are good on a wide variety of tasks, categorisation, extracting entities
:::

## How LLMs can help social scientists

- Writing assistants
- Preprocessing datasets, eg. by summarising or generating keywords
- Creation of synthetic datasets for piloting research or preserving privacy
- Categorising text or extracting particular features from it

::: {style="font-size: 50%;"}
Source: Ziems, Caleb et al. "Can Large Language Models Transform Computational Social Science?"
:::

## General requirements for concepts in content analysis
### Validating analysis requires people

- Core challenge for analysis is finding analytical concepts that
    1. Provide analytical value, i.e. reveal something insightful about data
    2. Are clear enough so that multiple people will agree on them
    3. Can be operationalised so that models such as LLMs can apply them

- Agreement between people is proxy for validity of concepts - for some research, codebook verified before modelling
- Finding right concepts may require many attempts, and LLMs may make iterative process easier

# Different workflows with LLMs {background-color="#40666e"}

## Typical workflow for modelling work

![](Pasted image 20230824122151.png)

Typical workflow: 

- You train your own model
- ..with an architecture specific to the problem
- ..and exclusively on your own data

<!--
So this is 
How exactly do LLMs fit into a typical research process?

Notable:
https://www.researchgate.net/figure/A-four-step-predictive-modeling-workflow-1-Data-preparation-includes-cleaning-and_fig2_342520450
- You train your own model
- Feature extraction and model architecture are bespoke for your problem
- Feature extraction: bag of words, word vectors
- Models: regression, naive bayes
-->

---

## Potential changes in workflows with LLMs

![](Pasted image 20230823151209.png)

- LLMs are "Swiss army knives": Single models can be applied for many different tasks
- A potential simplification in expertise required for modeling - but at a cost!

::: {.notes}
- Transfer learning: Models trained on large corpora can be applied in other domains

- Typically, when working with large datasets, a researcher would have to find a specific model

- Language models are decent at many things - standardisation in the modelling phase of research

- Creates other challenges - dependency on companies that pretrain models, ethical dilemmas

- General tool, simplifies the process of model selection

- Though there are several LLMs too! And computationally intensive, so it's like a tradeoff
:::

<!--
- Sometimes SOTA, sometimes specific models 
- Swiss-army knives like bundling
- General tool, simplifies the process of model selection
- Though there are several LLMs too! And computationally intensive, so it's like a tradeoff
-->

---

## 3 ways to do content analysis

::: {style="font-size: 75%;"}

|                       | Training your own model                                               | Supervised text categorisation with LLMs | Zero/Few-shot annotation with LLMs                |   |
|-----------------------|-----------------------------------------------------------------------|-------------------------------------|---------------------------------------------------|---|
| **Models used**       | Model chosen for task                                                 | LLM of choice                       | Typically LLM that can be prompted with instructions                                     |   |
| **Preparing model**   | Training model on your own dataset                                    | Finetuning existing LLM             | Preparing and testing prompts that instruct model |   |
| **Data requirements** | Training and validation dataset (potentially large dataset necessary) | Training and validation dataset     | Just validation dataset                           |   |

:::

<!--
Not black and white, but with supervised text cat, we are working with LLMs where we just feed in the data
With Zero-shot, we have LLMs that read instructions, that may include data
-->

# An example of supervised text categorisation {background-color="#40666e"}

---

## Hate speech research for Cabinet Office

![](Pasted image 20230830123704.png)

::: {.notes}
- Some collaborations with people in the panel
- Two projects funded by Cabinet office
- Research questions such as: which politicians receive abuse on social media, that require this kind of 
:::

## Definition of "hateful speech"

![](Pasted image 20230823154547.png)

**hateful speech**: Messages that 

  1. Contains slur from pre-defined list
  2. Use slurs used in abusive or threatening way
  
::: {.notes}
- Problematic, rather static using a predefined word list
- For public-facing project, there's value having rather clear definition, and focusing on messages that are quite unambiguously wrong
:::

## Creating a dataset for training

![](prodigy.png)

<!--
Training set, your source of examples, that the machine will use to replicate the same form of categorisation on a larger corpus
Programming by example
-->
---

## Finetuning an LLM for text classification

![](Pasted image 20230823215930.png)

Finetuning is a process where an pre-trained model goes through a smaller training process to perform a specific task.

<!--
Here, this is a slightly awkward graph, but it's to describe the 
you've got two neural networks.
Data comes from the bottom, and then its processed one layer in the neural network after another.
In finetuning for classification, we add this output layer, to perform.
-->

---

## Finetuning a model in Python

```
model = AutoModelForSequenceClassification.from_pretrained(
    "TurkuNLP/bert-base-finnish-uncased-v1"
)

training_args = TrainingArguments(
    "nethate", evaluation_strategy="epoch", logging_steps=30
)
metric = load_metric("accuracy")

trainer = Trainer(
    args=training_args,
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)
trainer.train()

finetuned_finbert = pipeline(
    model=model, tokenizer=tokenizer, task="sentiment-analysis", return_all_scores=True
)
```

Goal: A "boring", boilerplate model

::: {style="font-size: 50%;"}
Source: https://github.com/AleksiKnuutila/nethate_classifier/blob/master/train.py
:::

::: {.notes}
- So this is Python code for finetuning a language model
- This might seem complex, but default, few parameters changed
- I thought for this work, it's appropriate to have an almost "boring" model
- Embodies the idea that the value is created through improving the training dataset that was used and finding the right analytical angles, not through some technical work of optimising the model
:::

## Categorisation results

![](Pasted image 20230824091656.png)

---

## Evaluating the performance of the classification

![](Pasted image 20230824091245.png)

---

## Examining how the model works

![](importance.png)

# An alternative workflow: zero-shot annotation {background-color="#40666e"}

---

## An alternative workflow: zero-shot annotation

- Can be done with LLMs that are trained to be directed with prompts
- Zero-shot: When the model is asked to perform a task that it hasn't specifically been trained on, i.e. to apply new categories that are described to it
- Few-shot: When the model is provided with a limited number of examples for its task
- Results tend to be worse than with finetuned models

::: {style="font-size: 50%;"}
Source: Pangakis et al. "Automated Annotation with Generative AI Requires Validation"
:::

---

## Example of zero-shot annotation

![](Pasted image 20230824094514.png)

An example of annotating text: Assessing the amount of time passing in literary passages.

::: {style="font-size: 50%;"}
Source: Underwood, "Why Literary Time is Measured in Minutes"
:::

<!--
Literary-critical tradition suggested that there had been a pretty stable balance between “scene” (minute-by-minute description) and “summary” (which may cover weeks or years) until modernists started to leave out the summary and make every page breathlessly immediate
-->

---

## Prompting GPT-4 to read fiction

```
Read the following passage of fiction. Then do five things.
 
1: Briefly summarize the passage.

2: **Reason step by step to decide how much time is described in the passage. If the
passage doesn't include any explicit reference to time, you can guess how much 
time the events described would have taken**. Even description can imply the 
passage of time by describing the earlier history of people or buildings. But 
characters' references to the past or future in spoken dialogue should not count 
as time that passed in the scene. Report the time using units of years, weeks, 
days, hours, or minutes. Do not say zero or N/A.

3: If you described a range of possible times in step 2 take the midpoint of the
range. Then multiply to convert the units into minutes.

4: Report only the number of minutes elapsed, which should match the number in 
step 3. Do not reply N/A.

5: Given the amount of speculation required in step 2, describe your certainty 
about the estimate--either high, moderate, or low.

The passage follows: <omitting this to save space>
```

::: {style="font-size: 50%;"}
Source: https://tedunderwood.com/2023/03/19/using-gpt-4-to-measure-the-passage-of-time-in-fiction/
:::

---

## Results

```
1: A detective wakes up 'the next morning,' looks out a window for an undefined 
time, drinks (and presumably needs to make) two cups of coffee, then shaves and 
showers and gets dressed before stepping out his front door and seeing a car.

2: **Making coffee, showering, and getting dressed take at least an hour. There's 
some ambiguity about whether to count the implicit reference to yesterday 
(since this is 'the next morning') as time elapsed in the passage, but let's 
say no, since yesterday is not actually described. So, an hour to 90 minutes.**

3: 1.25 hours have elapsed. Multiplying by 60 minutes an hour that's 75 minutes.

4: 75 minutes.

5: Low confidence, because of ambiguity about a reference to the previous day.
```

---

## Findings

![](Pasted image 20230825100706.png)

"Watching the model think out loud about each passage actually gave me a better sense of how results were produced than I could extract from my own records of human research in 2017."

<!--



## Thematic analysis with LLMs?

- XXX
- Possible source: https://arxiv.org/pdf/2305.13014.pdf
- Possible source: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4487768

-->

<!--
## Simple guidelines for "prompt engineering"

- Giving examples
- Providing persona
- Trying to get machine-readable output can be a challenge
-->

# Brief summary {background-color="#40666e"}

## General challenges of working with LLMs

- Most recent powerful models closed, proprietary, opaque
- Proprietary models make research hard to reproduce
- Industry influence, centralisation (GPT-4 cost 100 million to train)
- Difficult to interpret (though models can be prompted to explain themselves)
- Sensitive data cannot be analysed with cloud services
- Ethics of models, eg. sourcing of their training data

## LLMs might help develop analytical concepts

:::: {.columns}

::: {.column width="50%" style="font-size: 80%;"}
- Finding right analytical concepts is a frequent challenge
- Previously the high cost of modelling meant concept development done on subset of data, before modelling
- LLMs may reduce cost of modelling and make it easier to iteratively read large datasets with novel concepts
:::

::: {.column width="50%" style="font-size: 50%;"}
![](Pasted image 20230824121403.png)

Source: Tracy, "A Phronetic Iterative Approach to Data Analysis in Qualitative Research"
:::

::::

::: {.notes}
- Bringing it together
- Finding the right concepts is hard
- Language models don't do this work for you, but they might provide new workflows for large datasets
- Previously research often in consecutive phases, first you agree on concepts, then you create the model
- The modelling is so costly that you can't repeat it
- Ideally you might try out whether the model can repeat your taxonomy, and see what kind of insights are gained
- And repeat this process
- Not uncommon as an idea in qualitative approaches
- Visualised, Tracy, phronetic approach, hermeneutic circle, defined in contrast with grounded theory..
:::

# Thanks! Any questions? {background-color="#40666e"}

Slides at http://knuutila.net/llm_methods

## Let's try to access CSC notebooks

- Go to the URL https://notebooks.rahtiapp.fi
- Log in with Haka account
- Press "Join workspace" in top right
- Enter code jyu-07ippvlk
- You may press power button icon to start your own virtual machine
- After waiting a moment, you can press in the same location to "enter session"

## Using Jupyter Labs

- After you have entered your session, you are inside of a system called Jupyter Labs, where you can view and execute Jupyter Notebooks
- On the left, you have list of files and folders. Double click on "jyu-llm-methods"
- There are two notebook files with filenames "Exercise_...". Double click both to open them
- Note that the virtual machine only lasts for 12 hours, and not all folders persistent

::: {.notes}
- Haven't used this in teaching!
- CSC just one platform for computation, notebooks can be run elsewhere too
:::

## 2 exercises

1. Text classification with supervised learning and finetuning an LLM
    - Example data: Yelp reviews with positive/negative classification
    - Some techniques for evaluating results, such as confusion matrices and "scattertext" for false classifications
    - Tasks: E.g. try out with other data, examine biases
2. Zero-shot text annotation by prompting an LLM
    - Example data: Russian and British news articles about Ukraine
    - Sample prompts for extracting sources of information
    - Tasks: E.g. Try out other prompts, examine model's mistakes
3. Text classification/annotation in ChatGPT
    - If you would rather not use the notebook interface or Python, feel free to think of applying other interfaces such as ChatGPT for your work

:: {.notes}
- Spirit of exercises: above all, try out something that is useful for you 
- Here are some resources that you can work with
:::